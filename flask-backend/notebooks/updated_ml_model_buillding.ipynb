{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5233617b-577d-4639-9307-2d9ffe7abae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "395cd629-d514-4150-b4a4-50ed10445dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../artifacts/mbti_1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b2c1b6-3dea-49a2-9938-f8a2c1a5ebb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce3c83b-4cff-4f43-9fed-3fb6b9e0f735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    INFJ\n",
       "1    ENTP\n",
       "2    INTP\n",
       "3    INTJ\n",
       "4    ENTJ\n",
       "Name: type, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a549697-dc9d-48a7-95aa-f89bfbf1cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mbti_mapping = {\n",
    "    'INFJ': 1, 'ENTP': 2, 'INTP': 3, 'INTJ': 4, 'ENTJ': 5, 'ENFJ': 6, 'INFP': 7,\n",
    "    'ENFP': 8, 'ISFP': 9, 'ISTP': 10, 'ISFJ': 11, 'ISTJ': 12, 'ESTP': 13, 'ESFP': 14,\n",
    "    'ESTJ': 15, 'ESFJ': 16\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751c1e2b-c51a-4f0a-98bc-a51d34433851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mbti_values(data, mapping):\n",
    "    data['type'] = data['type'].map(mapping)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9799db-0b22-4b7a-a323-c23e0f07d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = replace_mbti_values(data, mbti_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a286a9fd-eb2d-4155-8a87-d142e71bfb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       3\n",
       "3       4\n",
       "4       5\n",
       "       ..\n",
       "8670    9\n",
       "8671    8\n",
       "8672    3\n",
       "8673    7\n",
       "8674    7\n",
       "Name: type, Length: 8675, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e4cc60-62b4-4af6-b0c2-3fc01df28093",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c125c2a9-ffca-4d94-b80c-547806dab7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3df6591-da78-425f-9a4e-93ed842f614b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e54d63e-4e45-4a73-9abe-f2d3c4b84dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type     0\n",
       "posts    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048787a-a256-49df-93ab-7778f6031932",
   "metadata": {},
   "source": [
    "## Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a01e624-70d1-4016-8581-626fc0d259aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d19773d-43e3-4a58-94a6-e39d2c71e2a9",
   "metadata": {},
   "source": [
    " Convert uppercase to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3c2a027-e1dd-48dc-8076-e83302e5b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"posts\"] = data[\"posts\"].apply(lambda x: \" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3b08bd9-a402-4fc3-9240-ec7ee2d158c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    'http://www.youtube.com/watch?v=qsxhcwe3krw|||...\n",
       "1    'i'm finding the lack of me in these posts ver...\n",
       "2    'good one _____ https://www.youtube.com/watch?...\n",
       "3    'dear intp, i enjoyed our conversation the oth...\n",
       "4    'you're fired.|||that's another silly misconce...\n",
       "Name: posts, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"posts\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c992b57-98e5-4d54-abd0-0a08635f89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"posts\"] = data['posts'].apply(lambda x: \" \".join(re.sub(r'https?://\\S+', '', x) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead61415-6e2f-4c10-b618-75bf9a14dcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ' and intj moments  sportscenter not top ten p...\n",
       "1    'i'm finding the lack of me in these posts ver...\n",
       "2    'good one _____  course, to which i say i know...\n",
       "3    'dear intp, i enjoyed our conversation the oth...\n",
       "4    'you're fired.|||that's another silly misconce...\n",
       "Name: posts, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"posts\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784506f-5e19-4e0c-8ace-abfc539e4a0e",
   "metadata": {},
   "source": [
    "Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d0aee6-ab85-4746-ab1c-8ae481a258ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "data[\"posts\"] = data[\"posts\"].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff9d9c30-6ab6-4fb7-ab98-1f817a69c823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     and intj moments  sportscenter not top ten pl...\n",
       "1    im finding the lack of me in these posts very ...\n",
       "2    good one   course to which i say i know thats ...\n",
       "3    dear intp i enjoyed our conversation the other...\n",
       "4    youre firedthats another silly misconception t...\n",
       "Name: posts, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"posts\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a7264-1d09-4809-9d05-1d6e26df4004",
   "metadata": {},
   "source": [
    "Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0ca4ce9-55ac-4298-82e8-25d14e8ee84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"posts\"] = data['posts'].str.replace('\\\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51e236af-3a0f-4fb4-b41c-c239980e5d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     and intj moments  sportscenter not top ten pl...\n",
       "1    im finding the lack of me in these posts very ...\n",
       "2    good one   course to which i say i know thats ...\n",
       "3    dear intp i enjoyed our conversation the other...\n",
       "4    youre firedthats another silly misconception t...\n",
       "Name: posts, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"posts\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb2b5c-eca0-4544-9385-37198ea6eda1",
   "metadata": {},
   "source": [
    "Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df136ffb-e1ad-47f1-b470-5732bb4446b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a86f2d88-7a7a-43cb-894a-a9a7481b567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../static/model/corpora/stopwords/english', 'r') as file:\n",
    "    sw = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57e3cc92-e009-4f29-bba2-d63176be8684",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"posts\"] = data[\"posts\"].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bf76c33-fc0a-40c3-a64d-e34b58a3b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    intj moments sportscenter top ten plays pranks...\n",
       "1    im finding lack posts alarmingsex boring posit...\n",
       "2    good one course say know thats blessing cursed...\n",
       "3    dear intp enjoyed conversation day esoteric ga...\n",
       "4    youre firedthats another silly misconception a...\n",
       "Name: posts, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"posts\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaea26c-5f45-44d8-934d-8ff20f453d55",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c286a96-5335-4ae2-ab92-10ef34515b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fabed55-fdc5-465d-8458-c51416e6bc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['posts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef45c0ab-6169-44e2-b362-b99e1660c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"posts\"] = data[\"posts\"].apply(lambda x: \" \".join(ps.stem(x) for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51602b8e-a6ad-4f6b-a9d3-1925c4159c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    intj moment sportscent top ten play prankswhat...\n",
       "1    im find lack post alarmingsex bore posit often...\n",
       "2    good one cours say know that bless cursedo abs...\n",
       "3    dear intp enjoy convers day esoter gab natur u...\n",
       "4    your firedthat anoth silli misconcept approach...\n",
       "Name: posts, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"posts\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b6629af-42b4-4b2b-9cb7-ef616cf2b6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['posts'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a6fd8c-90a8-49e8-9f87-9d0c9849c6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993c6f8f-7f46-4f34-b2e5-e968ba864c82",
   "metadata": {},
   "source": [
    "### Building Vacabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acf72747-35c2-4509-a53c-5add3fe5b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d624776d-62d6-4e4a-af8b-339cd4c26a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in data['posts']:\n",
    "    vocab.update(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0fe6fc6-0c99-47e1-8694-23ba253b8f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273519"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5795e130-84a5-4b67-b563-d81113d18be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [key for key in vocab if vocab[key] > 2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "38689b3a-3c19-4f20-95fe-6e45a7bb8689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "743"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff2aa23c-464c-4d78-a1b6-6e0cbeb8428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w', encoding=\"utf-8\")\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "save_vocabulary(tokens, '../static/model/vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029f17b-662b-4e31-af80-e3f5d019bdfe",
   "metadata": {},
   "source": [
    "### Divide dataset to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "775a1e18-4130-4d61-97df-6a34416abcf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       3\n",
       "3       4\n",
       "4       5\n",
       "       ..\n",
       "8670    9\n",
       "8671    8\n",
       "8672    3\n",
       "8673    7\n",
       "8674    7\n",
       "Name: type, Length: 8675, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40e74419-18b8-4acc-a835-b8828f089cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[\"posts\"]\n",
    "y = data[\"type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c76688a-c839-48cc-a5ca-c95dbe826983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       intj moment sportscent top ten play prankswhat...\n",
       "1       im find lack post alarmingsex bore posit often...\n",
       "2       good one cours say know that bless cursedo abs...\n",
       "3       dear intp enjoy convers day esoter gab natur u...\n",
       "4       your firedthat anoth silli misconcept approach...\n",
       "                              ...                        \n",
       "8670    alway think cat fi dom reason websit becom neo...\n",
       "8671    soif thread alreadi exist someplac els heck de...\n",
       "8672    mani question thing would take purpl pill pick...\n",
       "8673    conflict right come want children honestli mat...\n",
       "8674    long sinc personalitycaf although doesnt seem ...\n",
       "Name: posts, Length: 8675, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3a2b34ec-7567-4ccc-a4c1-4f1e5f4c4828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       3\n",
       "3       4\n",
       "4       5\n",
       "       ..\n",
       "8670    9\n",
       "8671    8\n",
       "8672    3\n",
       "8673    7\n",
       "8674    7\n",
       "Name: type, Length: 8675, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c11812a-76d9-43be-bf05-771864a44366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b098831f-604e-4309-a454-073d11596882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6940,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c192b7c2-0eac-46f4-8154-ff9f041b8882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1735,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff76f49-3a14-4202-b884-efc1076ccac1",
   "metadata": {},
   "source": [
    "### Vectoriztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef615f22-32d2-4b20-a6c6-9546d7b6b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(ds, vocabulary):\n",
    "    vectorized_lst = []\n",
    "    \n",
    "    for sentence in ds:\n",
    "        sentence_lst = np.zeros(len(vocabulary))\n",
    "        \n",
    "        for i in range(len(vocabulary)):\n",
    "            if vocabulary[i] in sentence.split():\n",
    "                sentence_lst[i] = 1\n",
    "                \n",
    "        vectorized_lst.append(sentence_lst)\n",
    "        \n",
    "    vectorized_lst_new = np.asarray(vectorized_lst, dtype=np.float32)\n",
    "    \n",
    "    return vectorized_lst_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01c80094-b241-40f1-84a0-b4ce88d29fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_x_train = vectorizer(x_train, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30f5e38a-972c-4852-89ed-1f1f9c0cbb0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intj',\n",
       " 'moment',\n",
       " 'top',\n",
       " 'play',\n",
       " 'experi',\n",
       " 'life',\n",
       " 'perc',\n",
       " 'last',\n",
       " 'thing',\n",
       " 'infj',\n",
       " 'friend',\n",
       " 'post',\n",
       " 'next',\n",
       " 'day',\n",
       " 'rest',\n",
       " 'enfj',\n",
       " 'sorri',\n",
       " 'hear',\n",
       " 'natur',\n",
       " 'relationship',\n",
       " 'perfect',\n",
       " 'time',\n",
       " 'everi',\n",
       " 'exist',\n",
       " 'tri',\n",
       " 'figur',\n",
       " 'hard',\n",
       " 'welcom',\n",
       " 'stuff',\n",
       " 'game',\n",
       " 'set',\n",
       " 'least',\n",
       " 'minut',\n",
       " 'move',\n",
       " 'dont',\n",
       " 'mean',\n",
       " 'sit',\n",
       " 'mayb',\n",
       " 'come',\n",
       " 'three',\n",
       " 'youv',\n",
       " 'type',\n",
       " 'want',\n",
       " 'would',\n",
       " 'like',\n",
       " 'use',\n",
       " 'given',\n",
       " 'cognit',\n",
       " 'function',\n",
       " 'left',\n",
       " 'video',\n",
       " 'good',\n",
       " 'one',\n",
       " 'note',\n",
       " 'subject',\n",
       " 'complet',\n",
       " 'death',\n",
       " 'enfp',\n",
       " 'favorit',\n",
       " 'grow',\n",
       " 'current',\n",
       " 'cool',\n",
       " 'appear',\n",
       " 'late',\n",
       " 'someon',\n",
       " 'thought',\n",
       " 'confid',\n",
       " 'within',\n",
       " 'world',\n",
       " 'id',\n",
       " 'enjoy',\n",
       " 'worri',\n",
       " 'peopl',\n",
       " 'alway',\n",
       " 'around',\n",
       " 'entp',\n",
       " 'your',\n",
       " 'main',\n",
       " 'social',\n",
       " 'live',\n",
       " 'convers',\n",
       " 'even',\n",
       " 'realli',\n",
       " 'part',\n",
       " 'thread',\n",
       " 'high',\n",
       " 'eat',\n",
       " 'someth',\n",
       " 'follow',\n",
       " 'mani',\n",
       " 'could',\n",
       " 'think',\n",
       " 'watch',\n",
       " 'movi',\n",
       " 'class',\n",
       " 'noth',\n",
       " 'whole',\n",
       " 'reason',\n",
       " 'two',\n",
       " 'right',\n",
       " 'middl',\n",
       " 'today',\n",
       " 'happen',\n",
       " 'see',\n",
       " 'societi',\n",
       " 'everyon',\n",
       " 'becom',\n",
       " 'draw',\n",
       " 'idea',\n",
       " 'form',\n",
       " 'person',\n",
       " 'im',\n",
       " 'take',\n",
       " 'room',\n",
       " 'learn',\n",
       " 'share',\n",
       " 'much',\n",
       " 'kind',\n",
       " 'old',\n",
       " 'school',\n",
       " 'music',\n",
       " 'havent',\n",
       " 'heard',\n",
       " 'age',\n",
       " 'fail',\n",
       " 'speak',\n",
       " 'year',\n",
       " 'ago',\n",
       " 'ive',\n",
       " 'sort',\n",
       " 'better',\n",
       " 'posit',\n",
       " 'big',\n",
       " 'mental',\n",
       " 'he',\n",
       " 'way',\n",
       " 'start',\n",
       " 'new',\n",
       " 'find',\n",
       " 'lack',\n",
       " 'bore',\n",
       " 'often',\n",
       " 'exampl',\n",
       " 'creativ',\n",
       " 'isnt',\n",
       " 'that',\n",
       " 'word',\n",
       " 'hand',\n",
       " 'eye',\n",
       " 'test',\n",
       " 'score',\n",
       " 'internet',\n",
       " 'funni',\n",
       " 'respons',\n",
       " 'mention',\n",
       " 'believ',\n",
       " 'know',\n",
       " 'site',\n",
       " 'half',\n",
       " 'still',\n",
       " 'comment',\n",
       " 'sometim',\n",
       " 'go',\n",
       " 'quot',\n",
       " 'perhap',\n",
       " 'man',\n",
       " 'special',\n",
       " 'knowledg',\n",
       " 'power',\n",
       " 'rather',\n",
       " 'never',\n",
       " 'real',\n",
       " 'judg',\n",
       " 'ne',\n",
       " 'ti',\n",
       " 'domin',\n",
       " 'fe',\n",
       " 'emot',\n",
       " 'rare',\n",
       " 'si',\n",
       " 'also',\n",
       " 'ni',\n",
       " 'due',\n",
       " 'though',\n",
       " 'say',\n",
       " 'first',\n",
       " 'back',\n",
       " 'drive',\n",
       " 'look',\n",
       " 'best',\n",
       " 'make',\n",
       " 'lol',\n",
       " 'guy',\n",
       " 'hell',\n",
       " 'sound',\n",
       " 'manag',\n",
       " 'put',\n",
       " 'coupl',\n",
       " 'connect',\n",
       " 'awar',\n",
       " 'se',\n",
       " 'admit',\n",
       " 'get',\n",
       " 'w',\n",
       " 'heart',\n",
       " 'notic',\n",
       " 'known',\n",
       " 'ill',\n",
       " 'away',\n",
       " 'wont',\n",
       " 'anyth',\n",
       " 'style',\n",
       " 'great',\n",
       " 'song',\n",
       " 'long',\n",
       " 'love',\n",
       " 'close',\n",
       " 'extrovert',\n",
       " 'normal',\n",
       " 'book',\n",
       " 'estj',\n",
       " 'said',\n",
       " 'except',\n",
       " 'call',\n",
       " 'fear',\n",
       " 'anim',\n",
       " 'didnt',\n",
       " 'pretti',\n",
       " 'problem',\n",
       " 'femal',\n",
       " 'okay',\n",
       " 'help',\n",
       " 'develop',\n",
       " 'littl',\n",
       " 'describ',\n",
       " 'place',\n",
       " 'infp',\n",
       " 'hurt',\n",
       " 'turn',\n",
       " 'tell',\n",
       " 'typic',\n",
       " 'trait',\n",
       " 'list',\n",
       " 'seem',\n",
       " 'came',\n",
       " 'bad',\n",
       " 'alreadi',\n",
       " 'howev',\n",
       " 'deal',\n",
       " 'toward',\n",
       " 'easi',\n",
       " 'intp',\n",
       " 'identifi',\n",
       " 'imagin',\n",
       " 'bit',\n",
       " 'car',\n",
       " 'trust',\n",
       " 'theyr',\n",
       " 'weird',\n",
       " 'laugh',\n",
       " 'run',\n",
       " 'usual',\n",
       " 'leav',\n",
       " 'end',\n",
       " 'work',\n",
       " 'mine',\n",
       " 'meet',\n",
       " 'need',\n",
       " 'respond',\n",
       " 'wasnt',\n",
       " 'wouldnt',\n",
       " 'abl',\n",
       " 'sad',\n",
       " 'lose',\n",
       " 'knew',\n",
       " 'give',\n",
       " 'awesom',\n",
       " 'stupid',\n",
       " 'stay',\n",
       " 'opinion',\n",
       " 'theori',\n",
       " 'suggest',\n",
       " 'difficult',\n",
       " 'situat',\n",
       " 'random',\n",
       " 'open',\n",
       " 'glad',\n",
       " 'thank',\n",
       " 'made',\n",
       " 'sever',\n",
       " 'hour',\n",
       " 'line',\n",
       " 'later',\n",
       " 'keep',\n",
       " 'enough',\n",
       " 'cours',\n",
       " 'absolut',\n",
       " 'amaz',\n",
       " 'ye',\n",
       " 'case',\n",
       " 'feel',\n",
       " 'mind',\n",
       " 'truli',\n",
       " 'terribl',\n",
       " 'differ',\n",
       " 'face',\n",
       " 'accur',\n",
       " 'beauti',\n",
       " 'read',\n",
       " 'rememb',\n",
       " 'doubt',\n",
       " 'write',\n",
       " 'togeth',\n",
       " 'ever',\n",
       " 'entir',\n",
       " 'pick',\n",
       " 'human',\n",
       " 'felt',\n",
       " 'stori',\n",
       " 'recent',\n",
       " 'interest',\n",
       " 'includ',\n",
       " 'sens',\n",
       " 'touch',\n",
       " 'scare',\n",
       " 'true',\n",
       " 'fact',\n",
       " 'actual',\n",
       " 'hug',\n",
       " 'head',\n",
       " 'pictur',\n",
       " 'night',\n",
       " 'order',\n",
       " 'oh',\n",
       " 'hope',\n",
       " 'sleep',\n",
       " 'anyway',\n",
       " 'wish',\n",
       " 'well',\n",
       " 'may',\n",
       " 'wonder',\n",
       " 'issu',\n",
       " 'name',\n",
       " 'topic',\n",
       " 'final',\n",
       " 'skill',\n",
       " 'decid',\n",
       " 'care',\n",
       " 'what',\n",
       " 'understand',\n",
       " 'angri',\n",
       " 'quit',\n",
       " 'somewher',\n",
       " 'els',\n",
       " 'comfort',\n",
       " 'women',\n",
       " 'abil',\n",
       " 'chang',\n",
       " 'result',\n",
       " 'extrem',\n",
       " 'onlin',\n",
       " 'languag',\n",
       " 'art',\n",
       " 'studi',\n",
       " 'gener',\n",
       " 'import',\n",
       " 'must',\n",
       " 'show',\n",
       " 'istp',\n",
       " 'univers',\n",
       " 'rule',\n",
       " 'entj',\n",
       " 'deep',\n",
       " 'depend',\n",
       " 'individu',\n",
       " 'prefer',\n",
       " 'everyth',\n",
       " 'either',\n",
       " 'motiv',\n",
       " 'action',\n",
       " 'particularli',\n",
       " 'introvert',\n",
       " 'interact',\n",
       " 'alon',\n",
       " 'busi',\n",
       " 'anoth',\n",
       " 'probabl',\n",
       " 'seen',\n",
       " 'other',\n",
       " 'esfj',\n",
       " 'op',\n",
       " 'definit',\n",
       " 'mother',\n",
       " 'commun',\n",
       " 'let',\n",
       " 'got',\n",
       " 'sure',\n",
       " 'doesnt',\n",
       " 'necessarili',\n",
       " 'fine',\n",
       " 'break',\n",
       " 'sex',\n",
       " 'talk',\n",
       " 'lot',\n",
       " 'te',\n",
       " 'mom',\n",
       " 'fun',\n",
       " 'dark',\n",
       " 'logic',\n",
       " 'edit',\n",
       " 'far',\n",
       " 'appreci',\n",
       " 'level',\n",
       " 'detail',\n",
       " 'certain',\n",
       " 'self',\n",
       " 'god',\n",
       " 'annoy',\n",
       " 'dislik',\n",
       " 'us',\n",
       " 'observ',\n",
       " 'common',\n",
       " 'serious',\n",
       " 'control',\n",
       " 'mostli',\n",
       " 'mbti',\n",
       " 'answer',\n",
       " 'argument',\n",
       " 'second',\n",
       " 'refer',\n",
       " 'liter',\n",
       " 'happi',\n",
       " 'assum',\n",
       " 'youll',\n",
       " 'process',\n",
       " 'initi',\n",
       " 'accept',\n",
       " 'view',\n",
       " 'almost',\n",
       " 'bother',\n",
       " 'ask',\n",
       " 'intuit',\n",
       " 'done',\n",
       " 'listen',\n",
       " 'messag',\n",
       " 'point',\n",
       " 'math',\n",
       " 'number',\n",
       " 'saw',\n",
       " 'job',\n",
       " 'whatev',\n",
       " 'thei',\n",
       " 'agre',\n",
       " 'state',\n",
       " 'approach',\n",
       " 'month',\n",
       " 'crazi',\n",
       " 'lost',\n",
       " 'critic',\n",
       " 'shit',\n",
       " 'taken',\n",
       " 'project',\n",
       " 'gave',\n",
       " 'woman',\n",
       " 'possibl',\n",
       " 'went',\n",
       " 'marri',\n",
       " 'girl',\n",
       " 'forum',\n",
       " 'similar',\n",
       " 'question',\n",
       " 'colleg',\n",
       " 'xd',\n",
       " 'men',\n",
       " 'arent',\n",
       " 'brain',\n",
       " 'valu',\n",
       " 'instead',\n",
       " 'children',\n",
       " 'cri',\n",
       " 'drink',\n",
       " 'term',\n",
       " 'cannot',\n",
       " 'without',\n",
       " 'involv',\n",
       " 'worth',\n",
       " 'debat',\n",
       " 'vs',\n",
       " 'fi',\n",
       " 'polit',\n",
       " 'religion',\n",
       " 'goal',\n",
       " 'perceiv',\n",
       " 'object',\n",
       " 'neg',\n",
       " 'andi',\n",
       " 'fuck',\n",
       " 'hate',\n",
       " 'suck',\n",
       " 'remind',\n",
       " 'parent',\n",
       " 'wrong',\n",
       " 'energi',\n",
       " 'focu',\n",
       " 'met',\n",
       " 'scienc',\n",
       " 'inform',\n",
       " 'isfp',\n",
       " 'parti',\n",
       " 'found',\n",
       " 'although',\n",
       " 'bring',\n",
       " 'full',\n",
       " 'effect',\n",
       " 'sinc',\n",
       " 'avoid',\n",
       " 'truth',\n",
       " 'nice',\n",
       " 'express',\n",
       " 'couldnt',\n",
       " 'relat',\n",
       " 'told',\n",
       " 'respect',\n",
       " 'choic',\n",
       " 'stop',\n",
       " 'caus',\n",
       " 'kill',\n",
       " 'attract',\n",
       " 'surpris',\n",
       " 'anyon',\n",
       " 'took',\n",
       " 'consid',\n",
       " 'plan',\n",
       " 'begin',\n",
       " 'system',\n",
       " 'fall',\n",
       " 'along',\n",
       " 'english',\n",
       " 'degre',\n",
       " 'f',\n",
       " 'p',\n",
       " 'anxieti',\n",
       " 'research',\n",
       " 'tend',\n",
       " 'simpli',\n",
       " 'expect',\n",
       " 'explain',\n",
       " 'defin',\n",
       " 'practic',\n",
       " 'guess',\n",
       " 'text',\n",
       " 'isfj',\n",
       " 'father',\n",
       " 'istj',\n",
       " 'sister',\n",
       " 'side',\n",
       " 'cant',\n",
       " 'haha',\n",
       " 'past',\n",
       " 'yeah',\n",
       " 'futur',\n",
       " 'hold',\n",
       " 'older',\n",
       " 'brother',\n",
       " 'physic',\n",
       " 'whenev',\n",
       " 'whether',\n",
       " 'joke',\n",
       " 'origin',\n",
       " 'pay',\n",
       " 'specif',\n",
       " 'creat',\n",
       " 'tongu',\n",
       " 'stereotyp',\n",
       " 'strong',\n",
       " 'especi',\n",
       " 'attent',\n",
       " 'anymor',\n",
       " 'young',\n",
       " 'dream',\n",
       " 'less',\n",
       " 'fit',\n",
       " 'romant',\n",
       " 'free',\n",
       " 'desir',\n",
       " 'meant',\n",
       " 'stand',\n",
       " 'page',\n",
       " 'outsid',\n",
       " 'act',\n",
       " 'miss',\n",
       " 'add',\n",
       " 'purpos',\n",
       " 'wait',\n",
       " 'fight',\n",
       " 'lead',\n",
       " 'die',\n",
       " 'present',\n",
       " 'check',\n",
       " 'toi',\n",
       " 'kinda',\n",
       " 'pain',\n",
       " 'iti',\n",
       " 'low',\n",
       " 'direct',\n",
       " 'decis',\n",
       " 'goe',\n",
       " 'hous',\n",
       " 'somehow',\n",
       " 'friendship',\n",
       " 'huge',\n",
       " 'matter',\n",
       " 'might',\n",
       " 'ii',\n",
       " 'voic',\n",
       " 'affect',\n",
       " 'finish',\n",
       " 'forc',\n",
       " 'lie',\n",
       " 'earli',\n",
       " 'bodi',\n",
       " 'realiti',\n",
       " 'depress',\n",
       " 'seriou',\n",
       " 'food',\n",
       " 'certainli',\n",
       " 'advic',\n",
       " 'troubl',\n",
       " 'wear',\n",
       " 'sent',\n",
       " 'confus',\n",
       " 'quiet',\n",
       " 'hair',\n",
       " 'yet',\n",
       " 'chanc',\n",
       " 'super',\n",
       " 'cat',\n",
       " 'honest',\n",
       " 'short',\n",
       " 'countri',\n",
       " 'mood',\n",
       " 'realiz',\n",
       " 'easier',\n",
       " 'estp',\n",
       " 'there',\n",
       " 'date',\n",
       " 'black',\n",
       " 'she',\n",
       " 'walk',\n",
       " 'charact',\n",
       " 'teacher',\n",
       " 'dad',\n",
       " 'gonna',\n",
       " 'allow',\n",
       " 'excit',\n",
       " 'here',\n",
       " 'sensit',\n",
       " 'cold',\n",
       " 'light',\n",
       " 'insid',\n",
       " 'obvious',\n",
       " 'famili',\n",
       " 'younger',\n",
       " 'support',\n",
       " 'ignor',\n",
       " 'group',\n",
       " 'behavior',\n",
       " 'pleas',\n",
       " 'across',\n",
       " 'esfp',\n",
       " 'ok',\n",
       " 'male',\n",
       " 'suppos',\n",
       " 'basic',\n",
       " 'activ',\n",
       " 'ideal',\n",
       " 'buy',\n",
       " 'correct',\n",
       " 'curiou',\n",
       " 'intens',\n",
       " 'phone',\n",
       " 'qualiti',\n",
       " 'major',\n",
       " 'choos',\n",
       " 'exactli',\n",
       " 'week',\n",
       " 'discuss',\n",
       " 'partner',\n",
       " 'clear',\n",
       " 'dog',\n",
       " 'repli',\n",
       " 'boyfriend',\n",
       " 'afraid',\n",
       " 'spend',\n",
       " 'singl',\n",
       " 'unless',\n",
       " 'home',\n",
       " 'behind',\n",
       " 'appli',\n",
       " 'tire',\n",
       " 'kid',\n",
       " 'organ',\n",
       " 'amount',\n",
       " 'contact',\n",
       " 'awkward',\n",
       " 'base',\n",
       " 'step',\n",
       " 'simpl',\n",
       " 'nt',\n",
       " 'forget',\n",
       " 'struggl',\n",
       " 'opposit',\n",
       " 'child',\n",
       " 'frustrat',\n",
       " 'smile',\n",
       " 'sign',\n",
       " 'etc',\n",
       " 'stress',\n",
       " 'descript',\n",
       " 'total',\n",
       " 'money',\n",
       " 'small',\n",
       " 'honestli',\n",
       " 'easili',\n",
       " 'intellig',\n",
       " 'link',\n",
       " 'shi',\n",
       " 'enneagram',\n",
       " 'strang',\n",
       " 'particular',\n",
       " 'psycholog',\n",
       " 'constantli',\n",
       " 'n',\n",
       " 'space',\n",
       " 'hang',\n",
       " 'color',\n",
       " 'comput',\n",
       " 'perspect',\n",
       " 'nf',\n",
       " 'cute',\n",
       " 'hit',\n",
       " 'join',\n",
       " 'ai']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fa433a1-ff4b-48a9-a664-9d60076011ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 1., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "09fe3df1-5bb6-4348-b0de-89490834aea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_x_test = vectorizer(x_test, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5250a54-eef0-48ca-952c-20db9ad1ec67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5543     3\n",
       "5339     4\n",
       "4241     7\n",
       "8661     2\n",
       "1689     8\n",
       "        ..\n",
       "2221     4\n",
       "4583     4\n",
       "1234     2\n",
       "5434    10\n",
       "4142     4\n",
       "Name: type, Length: 1735, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "308b0553-2709-496f-91cd-e5793a17355c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1735, 743)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "20d45f94-82d1-49da-96ed-1a7af2ba85c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "7     1478\n",
       "1     1179\n",
       "3     1039\n",
       "4      859\n",
       "2      557\n",
       "8      525\n",
       "10     277\n",
       "9      216\n",
       "5      184\n",
       "12     157\n",
       "6      157\n",
       "11     135\n",
       "13      73\n",
       "14      36\n",
       "15      34\n",
       "16      34\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65bf588-e181-47f0-a2b0-6a09129fd874",
   "metadata": {},
   "source": [
    "### handle imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf662caa-89e6-47db-a021-3c522ca92030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23648, 743) (23648,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n",
    "vectorized_x_train_smote, y_train_smote = smote.fit_resample(vectorized_x_train, y_train)\n",
    "print(vectorized_x_train_smote.shape, y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b74ddb84-1485-424c-8f1f-1665662d0b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type\n",
       "7     1478\n",
       "1     1478\n",
       "4     1478\n",
       "8     1478\n",
       "2     1478\n",
       "3     1478\n",
       "11    1478\n",
       "12    1478\n",
       "6     1478\n",
       "10    1478\n",
       "15    1478\n",
       "13    1478\n",
       "9     1478\n",
       "5     1478\n",
       "16    1478\n",
       "14    1478\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_smote.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74106dc4-509f-484c-b08c-46c94dfa44f1",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5097459f-7944-4458-a079-3a8c3e10c9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9f9c1ae5-7768-4fe9-8220-fd89790a208c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def training_scores(y_act, y_pred):\n",
    "    acc = round(accuracy_score(y_act, y_pred), 3)\n",
    "    pr = round(precision_score(y_act, y_pred, average='macro'), 3)\n",
    "    rec = round(recall_score(y_act, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_act, y_pred, average='macro'), 3)\n",
    "    print(f'Training Scores:\\n\\tAccuracy = {acc}\\n\\tPrecision = {pr}\\n\\tRecall = {rec}\\n\\tF1-Score = {f1}')\n",
    "    \n",
    "def validation_scores(y_act, y_pred):\n",
    "    acc = round(accuracy_score(y_act, y_pred), 3)\n",
    "    pr = round(precision_score(y_act, y_pred, average='macro', zero_division=1), 3)\n",
    "    rec = round(recall_score(y_act, y_pred, average='macro'), 3)\n",
    "    f1 = round(f1_score(y_act, y_pred, average='macro'), 3)\n",
    "    print(f'Testing Scores:\\n\\tAccuracy = {acc}\\n\\tPrecision = {pr}\\n\\tRecall = {rec}\\n\\tF1-Score = {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74537f78-1dcc-42d4-a0c2-9ecc7be3bf0f",
   "metadata": {},
   "source": [
    "### Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0127c97-1c77-4716-89dd-b3ae631d6125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23648,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_smote.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "84cb1b47-8144-43a9-832a-509784432b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Training Scores:\n",
      "\tAccuracy = 0.957\n",
      "\tPrecision = 0.956\n",
      "\tRecall = 0.957\n",
      "\tF1-Score = 0.957\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.428\n",
      "\tPrecision = 0.285\n",
      "\tRecall = 0.295\n",
      "\tF1-Score = 0.286\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(vectorized_x_train_smote, y_train_smote)\n",
    "actual_iterations_lr = lr.n_iter_[0]\n",
    "max_iter_lr = actual_iterations_lr + 100\n",
    "lr = LogisticRegression(max_iter=max_iter_lr)\n",
    "lr.fit(vectorized_x_train_smote, y_train_smote)\n",
    "y_train_pred_lr = lr.predict(vectorized_x_train_smote)\n",
    "y_test_pred_lr = lr.predict(vectorized_x_test)\n",
    "print(\"Logistic Regression:\")\n",
    "training_scores(y_train_smote, y_train_pred_lr)\n",
    "validation_scores(y_test, y_test_pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999eaf41-2183-4320-a06f-19341b682eb0",
   "metadata": {},
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e6bd06c-87b6-448c-8cc4-7754a753473a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Multinomial Naive Bayes:\n",
      "Training Scores:\n",
      "\tAccuracy = 0.731\n",
      "\tPrecision = 0.755\n",
      "\tRecall = 0.731\n",
      "\tF1-Score = 0.74\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.367\n",
      "\tPrecision = 0.198\n",
      "\tRecall = 0.204\n",
      "\tF1-Score = 0.199\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(vectorized_x_train_smote, y_train_smote)\n",
    "y_train_pred_nb = nb.predict(vectorized_x_train_smote)\n",
    "y_test_pred_nb = nb.predict(vectorized_x_test)\n",
    "print(\"\\nMultinomial Naive Bayes:\")\n",
    "training_scores(y_train_smote, y_train_pred_nb)\n",
    "validation_scores(y_test, y_test_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a043da0-025f-4299-b38b-1fe19c0fd9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest:\n",
      "Training Scores:\n",
      "\tAccuracy = 1.0\n",
      "\tPrecision = 1.0\n",
      "\tRecall = 1.0\n",
      "\tF1-Score = 1.0\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.417\n",
      "\tPrecision = 0.618\n",
      "\tRecall = 0.186\n",
      "\tF1-Score = 0.187\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(vectorized_x_train_smote, y_train_smote)\n",
    "y_train_pred_rf = rf.predict(vectorized_x_train_smote)\n",
    "y_test_pred_rf = rf.predict(vectorized_x_test)\n",
    "print(\"\\nRandom Forest:\")\n",
    "training_scores(y_train_smote, y_train_pred_rf)\n",
    "validation_scores(y_test, y_test_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32276cae-453c-4c60-8ed5-5ce7353aab3b",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c452a2a6-90d5-4a85-aa6b-ca41b8aaf227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support Vector Machine (SVM):\n",
      "Training Scores:\n",
      "\tAccuracy = 0.997\n",
      "\tPrecision = 0.997\n",
      "\tRecall = 0.997\n",
      "\tF1-Score = 0.997\n",
      "Testing Scores:\n",
      "\tAccuracy = 0.497\n",
      "\tPrecision = 0.7\n",
      "\tRecall = 0.234\n",
      "\tF1-Score = 0.236\n"
     ]
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(vectorized_x_train_smote, y_train_smote)\n",
    "y_train_pred_svm = svm.predict(vectorized_x_train_smote)\n",
    "y_test_pred_svm = svm.predict(vectorized_x_test)\n",
    "print(\"\\nSupport Vector Machine (SVM):\")\n",
    "training_scores(y_train_smote, y_train_pred_svm)\n",
    "validation_scores(y_test, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fa25c40c-ba43-4851-b477-d16f87918324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../static/model/model.pickle', 'wb') as file:\n",
    "    pickle.dump(svm, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac141db6-04c2-457c-8b74-5b4ca764636a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
